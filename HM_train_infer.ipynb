{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad438e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:07.996235Z",
     "iopub.status.busy": "2022-05-06T10:19:07.996052Z",
     "iopub.status.idle": "2022-05-06T10:19:09.393401Z",
     "shell.execute_reply": "2022-05-06T10:19:09.392966Z"
    },
    "papermill": {
     "duration": 1.405512,
     "end_time": "2022-05-06T10:19:09.395222",
     "exception": false,
     "start_time": "2022-05-06T10:19:07.989710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "import catboost\n",
    "import faiss\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b5548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:09.415769Z",
     "iopub.status.busy": "2022-05-06T10:19:09.415621Z",
     "iopub.status.idle": "2022-05-06T10:19:12.649951Z",
     "shell.execute_reply": "2022-05-06T10:19:12.649416Z"
    },
    "papermill": {
     "duration": 3.238985,
     "end_time": "2022-05-06T10:19:12.651414",
     "exception": false,
     "start_time": "2022-05-06T10:19:09.412429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/workspace/h-and-m-personalized-fashion-recommendations'\n",
    "output_dir = './'\n",
    "transactions = pd.read_pickle(f\"{data_dir}/transactions_train.pkl\") \n",
    "users = pd.read_pickle(f\"{data_dir}/users.pkl\") \n",
    "items = pd.read_pickle(f\"{data_dir}/items.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946e787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:12.659219Z",
     "iopub.status.busy": "2022-05-06T10:19:12.659064Z",
     "iopub.status.idle": "2022-05-06T10:19:12.663907Z",
     "shell.execute_reply": "2022-05-06T10:19:12.663468Z"
    },
    "papermill": {
     "duration": 0.008972,
     "end_time": "2022-05-06T10:19:12.664719",
     "exception": false,
     "start_time": "2022-05-06T10:19:12.655747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_type = 'CatBoost' \n",
    "    popular_num_items = 60 \n",
    "    popular_weeks = 1 \n",
    "    train_weeks = 6 \n",
    "    item2item_num_items_for_same_product_code = 12 \n",
    "\n",
    "    # features\n",
    "    user_transaction_feature_weeks = 50 \n",
    "    item_transaction_feature_weeks = 16 \n",
    "    item_age_feature_weeks = 40 \n",
    "    user_volume_feature_weeks = 50 \n",
    "    item_volume_feature_weeks = 20 \n",
    "    user_item_volume_feature_weeks = 16 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e85f34",
   "metadata": {},
   "source": [
    "## 生成candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09378511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_candidates(transactions: pd.DataFrame, target_users: np.ndarray, week: int) -> pd.DataFrame:\n",
    "    print(f\"create candidates (week: {week})\")\n",
    "    assert len(target_users) == len(set(target_users)) \n",
    "\n",
    "    def create_candidates_repurchase(\n",
    "            strategy: str,  \n",
    "            transactions: pd.DataFrame, \n",
    "            target_users: np.ndarray, \n",
    "            week_start: int, \n",
    "            max_items_per_user: int=1234567890 \n",
    "        ) -> pd.DataFrame:\n",
    "        tr = transactions.query(\"user in @target_users and @week_start <= week\")[['user', 'item', 'week', 'day']].drop_duplicates(ignore_index=True) # 筛选指定user和week的交易样本\n",
    "\n",
    "        gr_day = tr.groupby(['user', 'item'])['day'].min().reset_index(name='day') \n",
    "        gr_week = tr.groupby(['user', 'item'])['week'].min().reset_index(name='week') \n",
    "        gr_volume = tr.groupby(['user', 'item']).size().reset_index(name='volume') \n",
    "\n",
    "        gr_day['day_rank'] = gr_day.groupby('user')['day'].rank() \n",
    "        gr_week['week_rank'] = gr_week.groupby('user')['week'].rank() \n",
    "        gr_volume['volume_rank'] = gr_volume.groupby('user')['volume'].rank(ascending=False) \n",
    "\n",
    "        candidates = gr_day.merge(gr_week, on=['user', 'item']).merge(gr_volume, on=['user', 'item']) \n",
    "\n",
    "        candidates['rank_meta'] = 10**9 * candidates['day_rank'] + candidates['volume_rank'] \n",
    "        candidates['rank_meta'] = candidates.groupby('user')['rank_meta'].rank(method='min') \n",
    "        \n",
    "        \n",
    "        candidates = candidates.query(\"rank_meta <= @max_items_per_user\").reset_index(drop=True) \n",
    "\n",
    "        candidates = candidates[['user', 'item', 'week_rank', 'volume_rank', 'rank_meta']].rename(columns={'week_rank': f'{strategy}_week_rank', 'volume_rank': f'{strategy}_volume_rank'})\n",
    "\n",
    "        candidates['strategy'] = strategy \n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def  create_candidates_popular(\n",
    "            transactions: pd.DataFrame, \n",
    "            target_users: np.ndarray, \n",
    "            week_start: int,\n",
    "            num_weeks: int, \n",
    "            num_items: int, \n",
    "        ) -> pd.DataFrame:\n",
    "\n",
    "        tr = transactions.query(\"@week_start <= week < @week_start + @num_weeks\")[['user', 'item']].drop_duplicates(ignore_index=True)\n",
    "        popular_items = tr['item'].value_counts().index.values[:num_items] \n",
    "        popular_items = pd.DataFrame({\n",
    "            'item': popular_items, \n",
    "            'rank': range(num_items), \n",
    "            'crossjoinkey': 1, \n",
    "        })\n",
    "\n",
    "        candidates = pd.DataFrame({\n",
    "            'user': target_users, \n",
    "            'crossjoinkey': 1,\n",
    "        })\n",
    "\n",
    "        candidates = candidates.merge(popular_items, on='crossjoinkey').drop('crossjoinkey', axis=1) \n",
    "        candidates = candidates.rename(columns={'rank': f'pop_rank'}) \n",
    "\n",
    "        candidates['strategy'] = 'pop' \n",
    "        return candidates.drop_duplicates(ignore_index=True)\n",
    "\n",
    "\n",
    "    def create_candidates_category_popular(\n",
    "        transactions: pd.DataFrame, \n",
    "        items: pd.DataFrame,\n",
    "        base_candidates: pd.DataFrame, \n",
    "        week_start: int, \n",
    "        num_weeks: int, \n",
    "        num_items_per_category: int, \n",
    "        category: str, \n",
    "    ) -> pd.DataFrame:\n",
    "        tr = transactions.query(\"@week_start <= week < @week_start + @num_weeks\")[['user', 'item']].drop_duplicates() \n",
    "        tr = tr.groupby('item').size().reset_index(name='volume') \n",
    "        tr = tr.merge(items[['item', category]], on='item') \n",
    "        tr['cat_volume_rank'] = tr.groupby(category)['volume'].rank(ascending=False, method='min') \n",
    "        tr = tr.query(\"cat_volume_rank <= @num_items_per_category\").reset_index(drop=True) \n",
    "        tr = tr[['item', category, 'cat_volume_rank']].reset_index(drop=True)\n",
    "\n",
    "        candidates = base_candidates[['user', 'item']].merge(items[['item', category]], on='item') \n",
    "        candidates = candidates.groupby(['user', category]).size().reset_index(name='cat_volume') \n",
    "        candidates = candidates.merge(tr, on=category).drop(category, axis=1)\n",
    "        candidates['strategy'] = 'cat_pop' \n",
    "        return candidates\n",
    "\n",
    "\n",
    "    candidates_repurchase = create_candidates_repurchase('repurchase', transactions, target_users, week)\n",
    "    candidates_popular = create_candidates_popular(transactions, target_users, week, CFG.popular_weeks, CFG.popular_num_items)\n",
    "    candidates_item2item2 = create_candidates_repurchase('item2item2', transactions, target_users, week, CFG.item2item_num_items_for_same_product_code) \n",
    "    candidates_dept = create_candidates_category_popular(transactions, items, candidates_item2item2, week, 1, 6, 'department_no_idx')\n",
    "\n",
    "    def drop_common_user_item(candidates_target: pd.DataFrame, candidates_reference: pd.DataFrame) -> pd.DataFrame:\n",
    "        tmp = candidates_reference[['user', 'item']].reset_index(drop=True) \n",
    "        tmp['flag'] = 1\n",
    "        candidates = candidates_target.merge(tmp, on=['user', 'item'], how='left') \n",
    "        return candidates.query(\"flag != 1\").reset_index(drop=True).drop('flag', axis=1) \n",
    "    candidates_dept = drop_common_user_item(candidates_dept, candidates_repurchase) \n",
    "    candidates = [\n",
    "        candidates_repurchase,\n",
    "        candidates_popular,\n",
    "        candidates_dept,\n",
    "    ]\n",
    "    candidates = pd.concat(candidates)\n",
    "    \n",
    "    print(f\"volume: {len(candidates)}\") \n",
    "    print(f\"duplicates: {len(candidates) / len(candidates[['user', 'item']].drop_duplicates())}\") \n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume').sort_values(by='volume', ascending=False).reset_index(drop=True)  \n",
    "    volumes['ratio'] = volumes['volume'] / volumes['volume'].sum()\n",
    "    print(volumes)\n",
    "\n",
    "    meta_columns = [c for c in candidates.columns if c.endswith('_meta')]\n",
    "    return candidates.drop(meta_columns, axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c3bcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:19:24.541958Z",
     "iopub.status.busy": "2022-05-06T10:19:24.541827Z",
     "iopub.status.idle": "2022-05-06T10:25:06.980533Z",
     "shell.execute_reply": "2022-05-06T10:25:06.979990Z"
    },
    "papermill": {
     "duration": 342.444439,
     "end_time": "2022-05-06T10:25:06.982031",
     "exception": false,
     "start_time": "2022-05-06T10:19:24.537592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidates = []\n",
    "for week in range(1+CFG.train_weeks): \n",
    "    target_users = transactions.query(\"week == @week\")['user'].unique() \n",
    "    candidates.append(create_candidates(transactions, target_users, week+1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916e735",
   "metadata": {},
   "source": [
    "## 生成candidates对应的labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c04d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:06.995799Z",
     "iopub.status.busy": "2022-05-06T10:25:06.995636Z",
     "iopub.status.idle": "2022-05-06T10:25:07.002746Z",
     "shell.execute_reply": "2022-05-06T10:25:07.002242Z"
    },
    "papermill": {
     "duration": 0.013202,
     "end_time": "2022-05-06T10:25:07.003535",
     "exception": false,
     "start_time": "2022-05-06T10:25:06.990333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_labels(candidates: pd.DataFrame, week: int) -> pd.DataFrame:\n",
    "    print(f\"merge labels (week: {week})\") \n",
    "    labels = transactions[transactions['week'] == week][['user', 'item']].drop_duplicates(ignore_index=True) \n",
    "    original_positives = len(labels) \n",
    "    labels['y'] = 1\n",
    "    labels = candidates.merge(labels, on=['user', 'item'], how='left')\n",
    "    labels['y'] = labels['y'].fillna(0)\n",
    "    \n",
    "    remaining_positives_total = labels[['user', 'item', 'y']].drop_duplicates(ignore_index=True)['y'].sum() \n",
    "    recall = remaining_positives_total / original_positives \n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    volumes = candidates.groupby('strategy').size().reset_index(name='volume') \n",
    "    remaining_positives = labels.groupby('strategy')['y'].sum().reset_index() \n",
    "    remaining_positives = remaining_positives.merge(volumes, on='strategy') \n",
    "    remaining_positives['recall'] = remaining_positives['y'] / original_positives \n",
    "    remaining_positives['hit_ratio'] = remaining_positives['y'] / remaining_positives['volume'] \n",
    "    remaining_positives = remaining_positives.sort_values(by='y', ascending=False).reset_index(drop=True)\n",
    "    print(remaining_positives) \n",
    "\n",
    "    return labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b6fe2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:07.014981Z",
     "iopub.status.busy": "2022-05-06T10:25:07.014726Z",
     "iopub.status.idle": "2022-05-06T10:25:40.040842Z",
     "shell.execute_reply": "2022-05-06T10:25:40.040241Z"
    },
    "papermill": {
     "duration": 33.033513,
     "end_time": "2022-05-06T10:25:40.042225",
     "exception": false,
     "start_time": "2022-05-06T10:25:07.008712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(len(candidates)): \n",
    "    candidates[idx] = merge_labels(candidates[idx], idx) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96daef9b",
   "metadata": {},
   "source": [
    "## candidates数据 再处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312ddcb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:40.055471Z",
     "iopub.status.busy": "2022-05-06T10:25:40.055341Z",
     "iopub.status.idle": "2022-05-06T10:25:45.090300Z",
     "shell.execute_reply": "2022-05-06T10:25:45.089791Z"
    },
    "papermill": {
     "duration": 5.042865,
     "end_time": "2022-05-06T10:25:45.091630",
     "exception": false,
     "start_time": "2022-05-06T10:25:40.048765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in range(len(candidates)):\n",
    "    candidates[idx]['week'] = idx\n",
    "\n",
    "candidates_valid_all = candidates[0].copy()\n",
    "\n",
    "def drop_trivial_users(labels):\n",
    "\n",
    "    bef = len(labels)\n",
    "    df = labels[labels['user'].isin(labels[['user', 'y']].drop_duplicates().groupby('user').size().reset_index(name='sz').query(\"sz==2\").user)].reset_index(drop=True) # 保留同时用正面和负面样本的用户\n",
    "    aft = len(df)\n",
    "    print(f\"drop trivial queries: {bef} -> {aft}\") \n",
    "    return df\n",
    "    \n",
    "for idx in range(len(candidates)):\n",
    "    candidates[idx] = drop_trivial_users(candidates[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cac3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates columns = ['user', 'item', 'repurchase_week_rank', 'repurchase_volume_rank', 'strategy', 'pop_rank', 'cat_volume', 'cat_volume_rank', 'y', 'week']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc49f96",
   "metadata": {},
   "source": [
    "## attach features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ec148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_embeddings(model_type: str, week: int, dim: int):\n",
    "    with open(f\"{data_dir}/lfm/lfm_{model_type}_week{week}_dim{dim}_model.pkl\", 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    biases, embeddings = model.get_user_representations(None)\n",
    "    n_user = len(biases) \n",
    "    a = np.hstack([embeddings, biases.reshape(n_user, 1)]) \n",
    "    user_embeddings = pd.DataFrame(a, columns=[f\"user_rep_{i}\" for i in range(dim + 1)]) \n",
    "    user_embeddings = pd.concat([pd.DataFrame({'user': range(n_user)}), user_embeddings], axis=1) \n",
    "\n",
    "    return user_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da29079a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:25:45.104440Z",
     "iopub.status.busy": "2022-05-06T10:25:45.104336Z",
     "iopub.status.idle": "2022-05-06T10:41:06.155659Z",
     "shell.execute_reply": "2022-05-06T10:41:06.155115Z"
    },
    "papermill": {
     "duration": 921.057729,
     "end_time": "2022-05-06T10:41:06.157206",
     "exception": false,
     "start_time": "2022-05-06T10:25:45.099477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def attach_features(transactions: pd.DataFrame, users: \n",
    "                    pd.DataFrame, items: pd.DataFrame, candidates: pd.DataFrame, week: int, pretrain_week: int) -> pd.DataFrame:\n",
    "    print(f\"attach features (week: {week})\")\n",
    "    n_original = len(candidates) \n",
    "    df = candidates.copy() \n",
    "\n",
    "    df = df.merge(users[['user', 'age']], on='user')\n",
    "\n",
    "    item_features = [c for c in items.columns if c.endswith('idx')]\n",
    "    df = df.merge(items[['item'] + item_features], on='item')\n",
    "\n",
    "    week_end = week + CFG.user_transaction_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('user')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['user_' + '_'.join(a) for a in tmp.columns.to_flat_index()] \n",
    "    df = df.merge(tmp, on='user', how='left') \n",
    "\n",
    "    week_end = week + CFG.item_transaction_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item')[['price', 'sales_channel_id']].agg(['mean', 'std'])\n",
    "    tmp.columns = ['item_' + '_'.join(a) for a in tmp.columns.to_flat_index()] \n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    week_end = week + CFG.item_age_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").merge(users[['user', 'age']], on='user')\n",
    "    tmp = tmp.groupby('item')['age'].agg(['mean', 'std']) \n",
    "    tmp.columns = [f'age_{a}' for a in tmp.columns.to_flat_index()] \n",
    "    df = df.merge(tmp, on='item', how='left') \n",
    "\n",
    "    tmp = transactions.query(\"@week <= week\").groupby('item')['day'].min().reset_index(name='item_day_min') \n",
    "    tmp['item_day_min'] -= transactions.query(\"@week == week\")['day'].min() \n",
    "    df = df.merge(tmp, on='item', how='left') \n",
    "\n",
    "    week_end = week + CFG.item_volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('item').size().reset_index(name='item_volume')\n",
    "    df = df.merge(tmp, on='item', how='left')\n",
    "\n",
    "    tmp = transactions.query(\"@week <= week\").groupby('user')['day'].min().reset_index(name='user_day_min') \n",
    "    tmp['user_day_min'] -= transactions.query(\"@week == week\")['day'].min() \n",
    "    df = df.merge(tmp, on='user', how='left')\n",
    "\n",
    "    week_end = week + CFG.user_volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby('user').size().reset_index(name='user_volume')\n",
    "    df = df.merge(tmp, on='user', how='left') \n",
    "\n",
    "    tmp = transactions.query(\"@week <= week\").groupby(['user', 'item'])['day'].min().reset_index(name='user_item_day_min') \n",
    "    tmp['user_item_day_min'] -= transactions.query(\"@week == week\")['day'].min() \n",
    "    df = df.merge(tmp, on=['item', 'user'], how='left') \n",
    "\n",
    "    week_end = week + CFG.user_item_volume_feature_weeks\n",
    "    tmp = transactions.query(\"@week <= week < @week_end\").groupby(['user', 'item']).size().reset_index(name='user_item_volume')\n",
    "    df = df.merge(tmp, on=['user', 'item'], how='left')\n",
    "\n",
    "    seen_users = transactions.query(\"week >= @pretrain_week\")['user'].unique() \n",
    "    user_reps = calc_embeddings('i_i', pretrain_week, 16) \n",
    "    user_reps = user_reps.query(\"user in @seen_users\") \n",
    "    df = df.merge(user_reps, on='user', how='left') \n",
    "\n",
    "    assert len(df) == n_original\n",
    "    return df\n",
    "\n",
    "dataset_oof = attach_features(transactions, users, items, candidates_valid_all, 1, CFG.train_weeks+1)\n",
    "datasets_train_valid = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks+1) for idx in range(len(candidates))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a6b4e",
   "metadata": {},
   "source": [
    "## 划分Train/Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be4d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:41:06.170801Z",
     "iopub.status.busy": "2022-05-06T10:41:06.170674Z",
     "iopub.status.idle": "2022-05-06T10:41:42.423578Z",
     "shell.execute_reply": "2022-05-06T10:41:42.423126Z"
    },
    "papermill": {
     "duration": 36.26077,
     "end_time": "2022-05-06T10:41:42.425367",
     "exception": false,
     "start_time": "2022-05-06T10:41:06.164597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " for idx in range(len(datasets_train_valid)): \n",
    "    datasets_train_valid[idx]['query_group'] = datasets_train_valid[idx]['week'].astype(str) + '_' + datasets_train_valid[idx]['user'].astype(str) # 新列 query_group = \"week_user\"\n",
    "    datasets_train_valid[idx] = datasets_train_valid[idx].sort_values(by='query_group').reset_index(drop=True) \n",
    "\n",
    "def concat_train(datasets_train_valid, begin, num):\n",
    "    train = pd.concat([datasets_train_valid[idx] for idx in range(begin, begin+num)])\n",
    "    return train\n",
    "\n",
    "train = concat_train(datasets_train_valid, 1, CFG.train_weeks) \n",
    "valid = datasets_train_valid[0] # valid data\n",
    "\n",
    "feature_columns = [c for c in valid.columns if c not in ['y', 'strategy', 'query_group', 'week']] # \n",
    "print(f\"feature_columns:\\n{feature_columns}\") \n",
    "\n",
    "cat_feature_values = [c for c in feature_columns if c.endswith('idx')] \n",
    "cat_features = [feature_columns.index(c) for c in cat_feature_values] \n",
    "print(f\"\\ncat_feature_values:\\n{cat_feature_values}\")\n",
    "print(f\"\\ncat_features:\\n{cat_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc898165",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a2a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:41:45.639836Z",
     "iopub.status.busy": "2022-05-06T10:41:45.639725Z",
     "iopub.status.idle": "2022-05-06T10:41:45.643141Z",
     "shell.execute_reply": "2022-05-06T10:41:45.642750Z"
    },
    "papermill": {
     "duration": 0.010072,
     "end_time": "2022-05-06T10:41:45.643843",
     "exception": false,
     "start_time": "2022-05-06T10:41:45.633771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_query_group(df):\n",
    "    users = df['user'].values \n",
    "    comp_seq_index, = np.concatenate(([True], users[1:]!=users[:-1], [True])).nonzero() \n",
    "    group = list(np.ediff1d(comp_seq_index)) \n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca57bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T10:41:45.655899Z",
     "iopub.status.busy": "2022-05-06T10:41:45.655781Z",
     "iopub.status.idle": "2022-05-06T14:02:45.953080Z",
     "shell.execute_reply": "2022-05-06T14:02:45.952670Z"
    },
    "papermill": {
     "duration": 12060.305311,
     "end_time": "2022-05-06T14:02:45.954897",
     "exception": false,
     "start_time": "2022-05-06T10:41:45.649586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train) \n",
    "    group_valid = get_query_group(valid)\n",
    "\n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train)\n",
    "    valid_dataset = lgb.Dataset(valid[feature_columns], valid['y'], group=group_valid, reference=train_dataset) \n",
    "\n",
    "    params = {\n",
    "        'objective': 'xendcg',\n",
    "        'boosting_type': 'gbdt', \n",
    "        'learning_rate': 1e-6,\n",
    "        'num_leaves': 255, \n",
    "        'min_data_in_leaf': 100, \n",
    "        'metric': 'map',\n",
    "        'eval_at': 12,\n",
    "    }\n",
    "\n",
    "    model = lgb.train(\n",
    "                        params,\n",
    "                        train_dataset, \n",
    "                        valid_sets=[train_dataset, valid_dataset],\n",
    "                        num_boost_round=1000, \n",
    "                        callbacks=[lgb.early_stopping(20)] \n",
    "                    )\n",
    "\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16)) \n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_dataset = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "    valid_dataset = catboost.Pool(data=valid[feature_columns], label=valid['y'], group_id=valid['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params = {\n",
    "        'loss_function': 'YetiRank',\n",
    "        'use_best_model': True,\n",
    "        'one_hot_max_size': 300,\n",
    "        'iterations': 10000,\n",
    "    }\n",
    "    model = catboost.CatBoost(params) \n",
    "    model.fit(train_dataset, eval_set=valid_dataset)\n",
    "\n",
    "    plt.plot(model.get_evals_result()['validation']['PFound'])\n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_dataset)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])\n",
    "\n",
    "del train, valid, train_dataset, valid_dataset\n",
    "gc.collect()\n",
    "\n",
    "with open(f'{output_dir}/model_for_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115baf96",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a205b21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-06T14:02:46.227921Z",
     "iopub.status.busy": "2022-05-06T14:02:46.227763Z",
     "iopub.status.idle": "2022-05-06T14:03:56.003654Z",
     "shell.execute_reply": "2022-05-06T14:03:56.003164Z"
    },
    "papermill": {
     "duration": 70.04725,
     "end_time": "2022-05-06T14:03:56.140916",
     "exception": false,
     "start_time": "2022-05-06T14:02:46.093666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = dataset_oof[['user', 'item']].reset_index(drop=True) \n",
    "pred['pred'] = model.predict(dataset_oof[feature_columns]) \n",
    "\n",
    "pred = pred.groupby(['user', 'item'])['pred'].max().reset_index() \n",
    "pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index() \n",
    "\n",
    "gt = transactions.query(\"week == 0\").groupby('user')['item'].apply(list).reset_index().rename(columns={'item': 'gt'})\n",
    "merged = gt.merge(pred, on='user', how='left') \n",
    "merged['item'] = merged['item'].fillna('').apply(list) \n",
    "\n",
    "merged.to_pickle(f'{output_dir}/merged_100.pkl') \n",
    "dataset_oof.to_pickle(f'{output_dir}/valid_all_100.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913199d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "def apk(actual, predicted, k=12):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k.\n",
    "\n",
    "    This function computes the average prescision at k between two lists of\n",
    "    items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of elements that are to be predicted (order doesn't matter)\n",
    "    predicted : list\n",
    "                A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k] \n",
    "\n",
    "    score = 0.0 \n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]: \n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0 \n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "\n",
    "def mapk(actual, predicted, k=12):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k.\n",
    "\n",
    "    This function computes the mean average prescision at k between two lists\n",
    "    of lists of items.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actual : list\n",
    "             A list of lists of elements that are to be predicted\n",
    "             (order doesn't matter in the lists)\n",
    "    predicted : list\n",
    "                A list of lists of predicted elements\n",
    "                (order matters in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : double\n",
    "            The mean average precision at k over the input lists\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "\n",
    "print('mAP@12:', mapk(merged['gt'], merged['item']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f4a3e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22c127",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets = [attach_features(transactions, users, items, candidates[idx], 1+idx, CFG.train_weeks) for idx in range(len(candidates))]\n",
    "\n",
    "for idx in range(len(datasets)):\n",
    "    datasets[idx]['query_group'] = datasets[idx]['week'].astype(str) + '_' + datasets[idx]['user'].astype(str) \n",
    "    datasets[idx] = datasets[idx].sort_values(by='query_group').reset_index(drop=True) \n",
    "\n",
    "train = concat_train(datasets, 0, CFG.train_weeks) \n",
    "\n",
    "if CFG.model_type == 'LightGBM':\n",
    "    group_train = get_query_group(train) \n",
    "    train_dataset = lgb.Dataset(train[feature_columns], train['y'], group=group_train) \n",
    "\n",
    "    best_iteration = model.best_iteration \n",
    "    model = lgb.train(params, train_dataset, num_boost_round=best_iteration)\n",
    "    lgb.plot_importance(model, importance_type='gain', figsize=(8, 16))\n",
    "\n",
    "\n",
    "elif CFG.model_type == 'CatBoost':\n",
    "    train_dataset = catboost.Pool(data=train[feature_columns], label=train['y'], group_id=train['query_group'], cat_features=cat_features)\n",
    "\n",
    "    params['iterations'] = model.get_best_iteration()\n",
    "    params['use_best_model'] = False \n",
    "    model = catboost.CatBoost(params) \n",
    "    model.fit(train_dataset) \n",
    "\n",
    "    feature_importance = model.get_feature_importance(train_dataset)\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.yticks(range(len(feature_columns)), np.array(feature_columns)[sorted_idx])\n",
    "    plt.barh(range(len(feature_columns)), feature_importance[sorted_idx])\n",
    "\n",
    "\n",
    "del train, train_dataset\n",
    "gc.collect()\n",
    "with open(f'{output_dir}/model_for_submission.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "\n",
    "del datasets, dataset_oof, candidates, candidates_valid_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6c67f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_users = users['user'].values \n",
    "all_users \n",
    "preds = []\n",
    "\n",
    "n_split_prediction = 10\n",
    "n_chunk = (len(all_users) + n_split_prediction - 1)// n_split_prediction\n",
    "for i in range(0, len(all_users), n_chunk):\n",
    "    print(f\"chunk: {i}\")\n",
    "    target_users = all_users[i:i+n_chunk]\n",
    "\n",
    "    candidates = create_candidates(transactions, target_users, 0) \n",
    "    candidates = attach_features(transactions, users, items, candidates, 0, CFG.train_weeks) \n",
    "\n",
    "    candidates['pred'] = model.predict(candidates[feature_columns]) \n",
    "    pred = candidates.groupby(['user', 'item'])['pred'].max().reset_index() \n",
    "    pred = pred.sort_values(by=['user', 'pred'], ascending=False).reset_index(drop=True).groupby('user')['item'].apply(lambda x: list(x)[:12]).reset_index() \n",
    "    preds.append(pred)\n",
    "\n",
    "pred = pd.concat(preds).reset_index(drop=True) \n",
    "assert len(pred) == len(all_users)\n",
    "assert np.array_equal(pred['user'].values, all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c19607",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_user = pd.read_pickle(f\"{data_dir}/mp_customer_id.pkl\")\n",
    "mp_item = pd.read_pickle(f\"{data_dir}/mp_article_id.pkl\") # \n",
    "a_user = mp_user['val'].values \n",
    "a_item = mp_item['val'].values\n",
    "\n",
    "pred['customer_id'] = pred['user'].apply(lambda x: a_user[x])\n",
    "pred['prediction'] = pred['item'].apply(lambda x: list(map(lambda y: a_item[y], x)))\n",
    "pred['prediction'] = pred['prediction'].apply(lambda x: ' '.join(map(str, x))) \n",
    "\n",
    "submission = pred[['customer_id', 'prediction']] \n",
    "submission.to_csv('submission.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e7a7d138e1807293d1e3f8013295cd32ce1ecfefa78d2e112676f884ff9955c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13492.067398,
   "end_time": "2022-05-06T14:03:59.094934",
   "environment_variables": {},
   "exception": true,
   "input_path": "exp0.ipynb",
   "output_path": "exp0.out.ipynb",
   "parameters": {},
   "start_time": "2022-05-06T10:19:07.027536",
   "version": "2.3.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
